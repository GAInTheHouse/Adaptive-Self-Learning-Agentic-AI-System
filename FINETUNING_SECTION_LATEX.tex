\subsection{LoRA-Based Parameter-Efficient Fine-Tuning}

Our fine-tuning implementation incorporates LoRA (Low-Rank Adaptation) as a parameter-efficient fine-tuning strategy, designed to reduce computational costs and memory requirements while maintaining model performance. The system is built with modularity in mind, supporting both LoRA-based and full fine-tuning approaches depending on the model architecture and dataset characteristics.

\subsubsection{Implementation Architecture}

The fine-tuning module (\texttt{FineTuner} class) is implemented with comprehensive support for LoRA adapters using the PEFT (Parameter-Efficient Fine-Tuning) library. The implementation includes automatic detection of target modules for LoRA application, specifically identifying attention projection layers (query, key, value, and output projections) within transformer architectures. When LoRA is enabled, the system applies low-rank matrices to these attention modules, significantly reducing the number of trainable parameters while preserving the model's representational capacity.

The LoRA configuration uses default hyperparameters of rank $r=8$ and alpha $\alpha=16$, which provides a good balance between parameter efficiency and model performance. The system automatically calculates the percentage of trainable parameters, typically achieving a reduction to approximately 0.3\% of the total model parameters when LoRA is successfully applied. Additionally, the implementation ensures that task-specific heads (such as the CTC head for speech recognition) remain fully trainable even when LoRA is applied to the encoder layers, as these heads are critical for domain adaptation.

\subsubsection{Wav2Vec2 Compatibility and Limitations}

During implementation and testing, we discovered that LoRA cannot be effectively used with Wav2Vec2 models due to fundamental compatibility issues between the PEFT library and Wav2Vec2's CTC (Connectionist Temporal Classification) architecture. Specifically, when LoRA adapters are applied to Wav2Vec2 models, the forward pass produces NaN (Not-a-Number) values in the logits, rendering the model unusable. This issue stems from the interaction between PEFT's adapter mechanism and Wav2Vec2's unique input processing pipeline, which uses \texttt{input\_values} (raw audio features) rather than the \texttt{input\_ids} (token embeddings) that PEFT expects for language models.

To address this limitation while maintaining system efficiency, we implemented a selective fine-tuning strategy for Wav2Vec2 models. The encoder layers are frozen, and only the CTC head (\texttt{lm\_head}) is trained, which still provides effective domain adaptation while requiring minimal computational resources. This approach is particularly efficient for small datasets (less than 100 samples), as only a small fraction of the model parameters need to be updated. Additionally, we implemented numerical stability measures by disabling dropout layers and setting LayerNorm modules to evaluation mode during training, preventing NaN issues that can occur with Wav2Vec2's training dynamics.

It is important to note that LoRA support remains fully functional and is recommended for other STT model architectures that are compatible with PEFT, such as Whisper or other transformer-based models. The system automatically detects the model type and applies the appropriate fine-tuning strategy, ensuring optimal performance across different architectures.

\subsubsection{Training Pipeline and Data Processing}

The fine-tuning pipeline processes error cases collected from the self-learning system, where each sample consists of an audio file path and its corresponding corrected transcript (obtained from the LLM correction module). The audio data is preprocessed using the Wav2Vec2 processor, which resamples audio to 16kHz and extracts mel-spectrogram features. The transcripts are tokenized using the model's vocabulary, with proper handling of CTC-specific requirements such as padding tokens and label alignment.

The training process uses the HuggingFace \texttt{Trainer} API with custom data collators that handle variable-length audio sequences through dynamic padding. The system implements validation monitoring with configurable train-validation splits (default 20\%), early stopping based on validation accuracy, and overfitting detection mechanisms. Training hyperparameters include learning rates in the range of $1 \times 10^{-6}$ to $5 \times 10^{-5}$, batch sizes of 4-32, and configurable epoch counts, with gradient clipping to prevent numerical instability.

\subsubsection{Dataset and Audio Sources}

The audio files used for fine-tuning were sourced from the Edinburgh DataShare Noisy Speech Database \cite{edinburgh_noisy_speech}, available at \url{https://datashare.ed.ac.uk/handle/10283/2791}. This dataset provides parallel clean and noisy speech recordings at 48kHz, designed for training speech enhancement algorithms and text-to-speech models. The database includes recordings from the CSTR VCTK Corpus with various noise conditions, making it suitable for robust STT model training. The dataset's diverse acoustic conditions help the fine-tuned model generalize better to real-world scenarios with background noise and varying audio quality.

\subsubsection{LLM Correction Module: Llama Integration}

The LLM correction module utilizes Llama models (specifically Llama 3.2 3B) through the Ollama framework for generating corrected transcripts that serve as the gold standard during fine-tuning. The choice of Llama was motivated by several factors: (1) \textbf{Open-source availability}: Llama models are freely available and can be run locally via Ollama, ensuring data privacy and eliminating API costs; (2) \textbf{Performance-quality balance}: The 3B parameter variant provides excellent text correction capabilities while remaining computationally efficient for real-time inference; (3) \textbf{Modularity}: The implementation uses Ollama's standardized API, making the LLM component easily swappable with any other model supported by Ollama (including Llama 2, Mistral, Gemma, or custom models); (4) \textbf{Local deployment}: Running models locally via Ollama eliminates dependency on external APIs and provides consistent, low-latency inference.

The LLM corrector is implemented as a modular component (\texttt{LlamaLLMCorrector} class) that can be seamlessly replaced with alternative LLM backends. The system architecture abstracts the LLM interface, allowing users to swap between different models by simply changing the model name parameter. This design ensures that the fine-tuning system remains flexible and can adapt to different computational resources and quality requirements. For instance, users can opt for larger Llama models (7B or 13B) for higher quality corrections, or switch to lighter models for faster inference, all without modifying the core fine-tuning pipeline.

The correction process involves constructing context-aware prompts that include the original transcript, detected error patterns, and additional metadata (audio length, confidence scores). The LLM generates corrected transcripts with proper capitalization, punctuation, and grammatical improvements, which are then used as training targets for the fine-tuning process. This approach leverages the LLM's linguistic knowledge to create high-quality training data, even when the original STT output contains errors.
