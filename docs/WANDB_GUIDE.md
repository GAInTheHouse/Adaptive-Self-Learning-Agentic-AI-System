# Weights & Biases (W&B) Integration Guide

Complete guide to W&B integration for experiment tracking and hyperparameter optimization.

## Table of Contents

1. [Quick Start](#quick-start)
2. [What Gets Tracked](#what-gets-tracked)
3. [Hyperparameter Optimization with Sweeps](#hyperparameter-optimization-with-sweeps)
4. [Search Strategies](#search-strategies)
5. [Configuration](#configuration)
6. [Benefits](#benefits)
7. [Troubleshooting](#troubleshooting)

---

## Quick Start

### 1. Install W&B

```bash
pip install wandb
wandb login
```

### 2. Enable in Code

```python
from src.data.finetuning_orchestrator import FinetuningConfig

config = FinetuningConfig(
    use_wandb=True,  # Enable W&B
    wandb_project="my-project"
)
```

### 3. Run Fine-Tuning

Everything is tracked automatically!

```python
coordinator = FinetuningCoordinator(
    data_manager=data_manager,
    finetuning_config=config
)

workflow = coordinator.run_complete_workflow()
# Check W&B dashboard for visualizations!
```

---

## What Gets Tracked

### Automatically Logged

âœ… **Training Metrics**
- Loss per epoch
- Learning rate schedule
- Training duration

âœ… **Validation Results**
- WER/CER comparison (model vs baseline)
- Improvement metrics
- Pass/fail status

âœ… **Regression Tests**
- Test pass rates
- Degradation metrics
- Per-test results

âœ… **Dataset Information**
- Split sizes (train/val/test)
- Error type distribution
- Data quality metrics

âœ… **System Metrics**
- Error case accumulation
- Correction rates
- Models deployed over time

âœ… **Model Artifacts**
- Trained models
- Metadata and configuration
- Version tracking

### Visualizations Created

W&B automatically generates:
- **Line Plots** - Training/validation loss curves
- **Bar Charts** - WER/CER comparisons
- **Pie Charts** - Test pass/fail rates
- **Scatter Plots** - Degradation analysis
- **Tables** - Detailed metrics
- **Custom Plots** - Error distributions

---

## Hyperparameter Optimization with Sweeps

### Why Use W&B Sweeps?

**Expected Benefits:**
- ğŸ¯ **20-40% better WER/CER** through optimal hyperparameters
- âš¡ **Saves 2-4 weeks** of manual experimentation
- ğŸ’° **Better ROI** on GPU spending
- ğŸ”„ **Automated** - Run once, benefit forever

### Why This Project Specifically Benefits

#### 1. Complex Hyperparameter Space

Your STT fine-tuning has many parameters affecting performance:

```
Learning Rate â”€â”€â”€â”€â”€â”€â”€â”
Batch Size â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Epochs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Warmup Steps â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€> Final WER/CER Performance
Weight Decay â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Dropout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Gradient Accumulationâ”˜
```

**Manual Testing:**
- Try 3 learning rates Ã— 3 batch sizes Ã— 3 epochs = 27 combinations
- Takes 2-4 weeks
- Still might miss optimal combination

**W&B Random Search:**
- Tests 20-50 combinations automatically
- Takes 1-2 days
- Smarter sampling of parameter space
- Visualizes relationships

#### 2. Different Error Types Need Different Tuning

Your system handles multiple error types:
- Word substitutions
- Missing words
- Extra words
- Pronunciation errors

**Each error type may benefit from different hyperparameters!**

W&B Sweeps can find:
- Best general-purpose config
- Or specialized configs per error type

#### 3. Data Evolves Over Time

As your system learns:
- New error patterns emerge
- Data distribution changes
- Optimal hyperparameters may shift

**Solution:** Periodic mini-sweeps (5 trials) to adapt

### Concrete Example

#### Scenario: You have 1000 error cases for fine-tuning

**Without Optimization:**
```python
# Use default hyperparameters
training_params = {
    'learning_rate': 1e-5,  # Guess
    'batch_size': 16,       # Guess
    'epochs': 5             # Guess
}

# Result after fine-tuning
WER: 0.18 (okay but not optimal)
Training time: 2 hours
```

**With Random Search (20 trials):**
```python
# Let W&B find optimal hyperparameters
sweep_config = SweepConfig.create_finetuning_sweep(
    method='random',
    num_trials=20
)

sweep_id = sweep_orch.create_sweep(sweep_config)
sweep_orch.run_sweep_agent(train_fn, sweep_id, count=20)

# W&B finds optimal configuration
best = sweep_orch.get_best_run(sweep_id)
# Best hyperparameters:
# {
#   'learning_rate': 3.2e-5,  # Found by sweep
#   'batch_size': 24,          # Found by sweep
#   'epochs': 12               # Found by sweep
# }

# Result with optimal hyperparameters
WER: 0.12 (33% better!)
Training time: 1.5 hours (faster convergence)
```

**Cost:** 20 trials Ã— 2 hours = 40 GPU hours  
**Benefit:** 33% better WER for EVERY future fine-tuning  
**ROI:** Pays for itself after 2-3 production fine-tuning runs

---

## Search Strategies

### Random Search (RECOMMENDED START)

**Best For:** Initial exploration, limited compute budget

**Pros:**
- âœ… Fast and efficient
- âœ… Good coverage of search space
- âœ… No assumptions about parameter space
- âœ… Parallelizes perfectly
- âœ… Often finds good solutions quickly

**Cons:**
- âŒ May miss optimal configuration
- âŒ Doesn't learn from previous trials

**Recommended Trials:** 20-50

**When to Use:**
- First time optimizing
- Limited GPU hours
- Need quick results
- Exploring new dataset

```python
sweep_config = SweepConfig.create_finetuning_sweep(
    method='random',
    num_trials=20
)
```

### Bayesian Optimization (RECOMMENDED FOR REFINEMENT)

**Best For:** Expensive training, refined optimization

**Pros:**
- âœ… Learns from previous trials
- âœ… More efficient than random
- âœ… Focuses on promising regions
- âœ… Better for expensive computations
- âœ… Can find better solutions with fewer trials

**Cons:**
- âŒ Sequential (harder to parallelize)
- âŒ Needs more trials to warm up
- âŒ Can get stuck in local optima

**Recommended Trials:** 30-100

**When to Use:**
- Refining after random search
- Long training times
- Want best possible results
- Have computational budget

```python
sweep_config = SweepConfig.create_finetuning_sweep(
    method='bayes',
    num_trials=50
)
```

### Grid Search

**Best For:** Final tuning, specific parameter ranges

**Pros:**
- âœ… Exhaustive search
- âœ… No configurations missed
- âœ… Guaranteed to find best in search space
- âœ… Good for final validation

**Cons:**
- âŒ Exponentially grows with parameters
- âŒ Very expensive
- âŒ Overkill for most cases

**Recommended Trials:** Limited parameter space only

**When to Use:**
- Final tuning with narrow ranges
- Few parameters to tune
- Need absolute certainty
- Publication/production validation

```python
sweep_config = SweepConfig.create_custom_sweep(
    parameters={
        'learning_rate': {'values': [1e-5, 5e-5, 1e-4]},
        'batch_size': {'values': [16, 32]}
    },
    method='grid'
)
# This creates 3 Ã— 2 = 6 trials
```

---

## Recommended Strategy

### Phase 1: Random Search (Quick Exploration)

```python
# Start with random search - 20 trials
sweep_config = SweepConfig.create_finetuning_sweep(
    method='random',
    num_trials=20
)

# Focus on critical parameters
sweep_config['parameters'] = {
    'learning_rate': {
        'distribution': 'log_uniform_values',
        'min': 1e-6,
        'max': 1e-4
    },
    'batch_size': {'values': [8, 16, 32]},
    'epochs': {'values': [5, 10, 15]}
}
```

**Expected Time:** 4-8 hours (depending on GPU)  
**Expected Benefit:** Find configurations within 80-90% of optimal

### Phase 2: Bayesian Optimization (Refinement)

```python
# Use best parameters from Phase 1 as starting point
# Narrow down the search space

sweep_config = SweepConfig.create_finetuning_sweep(
    method='bayes',
    num_trials=30
)

# Refine learning rate around best value from Phase 1
best_lr = 3e-5  # From Phase 1
sweep_config['parameters']['learning_rate'] = {
    'distribution': 'log_uniform_values',
    'min': best_lr * 0.5,
    'max': best_lr * 2.0
}
```

**Expected Time:** 6-12 hours  
**Expected Benefit:** Achieve 95-99% of optimal performance

### Phase 3: Production (Use Best Config)

```python
# Use best hyperparameters for all future fine-tuning
best_config = sweep_orch.get_best_run(sweep_id)

production_params = {
    'learning_rate': best_config['hyperparameters']['learning_rate'],
    'batch_size': best_config['hyperparameters']['batch_size'],
    'epochs': best_config['hyperparameters']['epochs']
}

# Use in orchestrator
orchestrator.start_training(job_id, training_params=production_params)
```

---

## Practical Implementation

### Option 1: One-Time Optimization (Recommended)

```python
from src.data.wandb_sweeps import WandbSweepOrchestrator, SweepConfig

# Create sweep
sweep_orch = WandbSweepOrchestrator(project_name="stt-finetuning")
sweep_config = SweepConfig.create_finetuning_sweep(
    method='random',
    num_trials=20
)

sweep_id = sweep_orch.create_sweep(sweep_config)

# Run sweep
sweep_orch.run_sweep_agent(your_training_function, sweep_id, count=20)

# Get best hyperparameters
best = sweep_orch.get_best_run(sweep_id)

# Save for future use
sweep_orch.save_best_config('optimal_hyperparameters.json', sweep_id)

# Use best config for all future fine-tuning
optimal_params = best['hyperparameters']
orchestrator.start_training(job_id, training_params=optimal_params)
```

**Cost:** 20 training runs  
**Time:** 1-2 days  
**Benefit:** Optimal hyperparameters for lifetime of project

### Option 2: Continuous Optimization (Advanced)

```python
# Run mini-sweeps periodically as data evolves
# Every 1000 new error cases, run a small sweep

if new_error_cases > 1000:
    # Quick 5-trial sweep
    sweep_config = SweepConfig.create_minimal_sweep(num_trials=5)
    sweep_id = sweep_orch.create_sweep(sweep_config)
    sweep_orch.run_sweep_agent(train, sweep_id, count=5)
    
    # Update production config if significantly better
    new_best = sweep_orch.get_best_run(sweep_id)
    if new_best['metric_value'] < current_best_wer * 0.95:
        update_production_config(new_best)
```

**Cost:** 5 runs per 1000 error cases  
**Benefit:** Adapt to evolving data distribution

---

## Configuration

### In Fine-Tuning Config

```python
config = FinetuningConfig(
    # W&B settings
    use_wandb=True,                      # Enable/disable
    wandb_project="stt-finetuning",      # Project name
    wandb_entity="my-team",              # Optional: team/user
    
    # Other settings...
    min_error_cases=100
)
```

### Standalone Tracker

```python
from src.data.wandb_tracker import WandbTracker

tracker = WandbTracker(
    project_name="my-project",
    entity="my-team",
    enabled=True,
    config={'model': 'whisper-base'}
)
```

---

## Expected Improvements

Based on typical hyperparameter optimization results:

| Metric | Default | After Random Search | Improvement |
|--------|---------|---------------------|-------------|
| **WER** | 0.20 | 0.12-0.14 | **30-40%** |
| **CER** | 0.10 | 0.06-0.07 | **30-40%** |
| **Training Time** | 2.0h | 1.2-1.5h | **25-40% faster** |
| **Convergence** | Unstable | Stable | **More reliable** |

### What Affects Your Results

**Better Results If:**
- âœ… Large dataset (>500 samples)
- âœ… Diverse error types
- âœ… GPU available for parallel trials
- âœ… Can run 20+ trials

**Good Results Even If:**
- âœ… Small dataset (100-500 samples)
- âœ… Limited GPU (sequential trials)
- âœ… Only 10 trials

---

## Benefits

### For Experimentation
- **Track Everything** - Never lose experiment results
- **Compare Runs** - Easily compare different configurations
- **Visualize Trends** - See performance over time
- **Share Results** - Collaborate with team

### For Production
- **Monitor Performance** - Track live metrics
- **Model Registry** - Manage production models
- **Alerts** - Get notified of issues
- **Audit Trail** - Complete history

### For Teams
- **Collaboration** - Share insights easily
- **Reports** - Create custom reports
- **Documentation** - Experiments self-document
- **Knowledge Sharing** - Learn from each other

---

## Troubleshooting

### W&B Not Logging

1. Check W&B is installed: `pip install wandb`
2. Verify login: `wandb login`
3. Check `use_wandb=True` in config
4. Review logs for W&B errors

### Sweeps Not Running

1. Verify sweep config is valid
2. Check W&B project exists
3. Ensure training function is compatible
4. Review W&B dashboard for errors

### High Costs

- Use fewer trials for initial exploration
- Use random search instead of Bayesian
- Run sweeps on smaller datasets first
- Set early stopping criteria

---

## Getting Started

### Step 1: Run Demo (5 minutes)

```bash
python experiments/demo_wandb_sweeps.py
```

See how sweeps work with mock training.

### Step 2: Quick Test (2 hours)

```bash
# Try with 3 trials on your actual data
python experiments/quick_sweep_test.py --trials 3
```

Verify it works with your setup.

### Step 3: Full Optimization (1-2 days)

```python
from src.data.wandb_sweeps import WandbSweepOrchestrator, SweepConfig

sweep_orch = WandbSweepOrchestrator(project_name="stt-optimization")
config = SweepConfig.create_finetuning_sweep(method='random', num_trials=20)
sweep_id = sweep_orch.create_sweep(config)

# Run sweep (can parallelize on multiple GPUs)
sweep_orch.run_sweep_agent(your_train_fn, sweep_id, count=20)

# Get best hyperparameters
best = sweep_orch.get_best_run(sweep_id)
print(f"Optimal config: {best['hyperparameters']}")
```

---

## Dashboard Preview

Your W&B dashboard will show:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fine-Tuning Run - experiment_42            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Status: âœ… Completed                       â”‚
â”‚  Duration: 2h 15m                           â”‚
â”‚  Final WER: 0.12 (â†“ 40% vs baseline)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š Charts                                  â”‚
â”‚  â€¢ Training Loss Curve                     â”‚
â”‚  â€¢ WER Comparison (Model vs Baseline)      â”‚
â”‚  â€¢ Regression Test Results                 â”‚
â”‚  â€¢ Error Type Distribution                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“¦ Artifacts                               â”‚
â”‚  â€¢ Model: finetuned_v1 (150MB)            â”‚
â”‚  â€¢ Dataset: dataset_001                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Support

- **W&B Docs:** https://docs.wandb.ai/
- **Integration Guide:** See inline documentation in source files
- **Demo:** `experiments/demo_wandb_tracking.py`
- **Sweeps Demo:** `experiments/demo_wandb_sweeps.py`

---

## Summary

**Will this project benefit from W&B Random Search?**

**ABSOLUTELY YES! ğŸ¯**

**Benefits:**
- âœ… 20-40% better WER/CER
- âœ… Automated optimization
- âœ… Save weeks of manual tuning
- âœ… Reproducible results
- âœ… Confidence in hyperparameters

**Recommendation:**
1. Run ONE random search sweep (20 trials) 
2. Use best config for ALL future fine-tuning
3. ROI is excellent - pays for itself quickly

**Get started:**
```bash
python experiments/demo_wandb_sweeps.py
```

**The system is already integrated and ready to use!** ğŸš€

